# Zen Master Fine-tuning Configuration for N150 (Single Wormhole Chip)
#
# Optimized for Wormhole single-chip development hardware.
# - Smaller batch size to fit in DRAM
# - No DDP (single device)
# - Conservative gradient accumulation
# - Training completes in 1-3 hours on N150

training_config:
  model_type: "llama"
  seed: 42
  batch_size: 8                    # N150: Conservative for DRAM limits
  validation_batch_size: 2
  num_epochs: 3                    # 3 epochs over ~200 examples
  max_steps: 500                   # Reasonable for fine-tuning
  learning_rate: 0.0001            # Lower LR for fine-tuning
  weight_decay: 0.01
  use_moreh_adamw: true
  use_kahan_summation: false
  use_clip_grad_norm: true
  clip_grad_norm_max_norm: 1.0
  gradient_accumulation_steps: 4   # Effective batch: 8 * 4 = 32
  eval_every: 50                   # Validate every 50 steps
  model_save_interval: 100         # Checkpoint every 100 steps
  tokenizer_type: "bpe"
  checkpoint_dir: "checkpoints_zen_n150"
  model_config: "model_configs/tinyllama.yaml"

scheduler_config:
  max_lr: 1e-4                     # Peak learning rate
  min_lr: 1e-5                     # Minimum LR at end
  warmup_steps: 50                 # 10% warmup
  hold_steps: 350                  # Hold at peak for most of training

eval_config:
  repetition_penalty: 1.0
  temperature: 0.0                 # Greedy decoding for validation
  top_k: 50
  top_p: 1.0

device_config:
  enable_ddp: False                # N150: Single chip, no DDP
  mesh_shape: [1, 1]               # 1x1 mesh (single device)
