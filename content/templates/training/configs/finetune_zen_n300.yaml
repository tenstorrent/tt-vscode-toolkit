# Zen Master Fine-tuning Configuration for N300 (Dual Wormhole Chips)
#
# Optimized for Wormhole dual-chip development hardware.
# - Larger batch size with DDP
# - Data parallelism across 2 chips
# - Faster training than N150
# - Training completes in 30-60 minutes on N300

training_config:
  model_type: "llama"
  seed: 42
  batch_size: 16                   # N300: 2x the N150 batch size
  validation_batch_size: 4
  num_epochs: 3                    # 3 epochs over ~200 examples
  max_steps: 500                   # Same steps, but faster per step
  learning_rate: 0.0001            # Lower LR for fine-tuning
  weight_decay: 0.01
  use_moreh_adamw: true
  use_kahan_summation: false
  use_clip_grad_norm: true
  clip_grad_norm_max_norm: 1.0
  gradient_accumulation_steps: 2   # Effective batch: 16 * 2 = 32
  eval_every: 50                   # Validate every 50 steps
  model_save_interval: 100         # Checkpoint every 100 steps
  tokenizer_type: "bpe"
  checkpoint_dir: "checkpoints_zen_n300"
  model_config: "model_configs/tinyllama.yaml"

scheduler_config:
  max_lr: 1e-4                     # Peak learning rate
  min_lr: 1e-5                     # Minimum LR at end
  warmup_steps: 50                 # 10% warmup
  hold_steps: 350                  # Hold at peak for most of training

eval_config:
  repetition_penalty: 1.0
  temperature: 0.0                 # Greedy decoding for validation
  top_k: 50
  top_p: 1.0

device_config:
  enable_ddp: True                 # N300: Enable DDP for dual chips
  mesh_shape: [1, 2]               # 1x2 mesh (2 devices in parallel)
