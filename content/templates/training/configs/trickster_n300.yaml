# Trickster Fine-tuning Configuration for N300 (Dual Wormhole Chips with DDP)
#
# Optimized for N300 dual-chip hardware with Data Parallel training.
# - Larger batch size utilizing dual chips
# - DDP enabled for distributed training
# - Faster training than N150
# - Training completes in 30-60 minutes on N300
#
# The Trickster model: A creative, flexible AI for explaining concepts
# Demonstrates principles applicable to any custom model

training_config:
  model_type: "llama"
  seed: 42
  batch_size: 16                   # N300: Larger batch with DDP
  validation_batch_size: 4
  num_epochs: 3                    # 3 epochs over ~50 examples
  max_steps: 500                   # Same as N150 but faster wall-clock time
  learning_rate: 0.0001            # Lower LR for fine-tuning (constant, no scheduler)
  weight_decay: 0.01
  use_moreh_adamw: true
  use_kahan_summation: false
  use_clip_grad_norm: true
  clip_grad_norm_max_norm: 1.0
  gradient_accumulation_steps: 2   # Effective batch: 16 * 2 = 32 (same as N150)
  eval_every: 50                   # Validate every 50 steps
  model_save_interval: 100         # Checkpoint every 100 steps
  tokenizer_type: "bpe"
  checkpoint_dir: "checkpoints_trickster_n300"
  model_config: "model_configs/tinyllama.yaml"

  # Logging configuration (tt-blacksmith pattern)
  log_level: "INFO"
  use_wandb: false                 # Optional experiment tracking
  wandb_project: "trickster-finetune"
  wandb_run_name: "n300-tinyllama-ddp"

  # Checkpoint strategy (tt-blacksmith pattern)
  checkpoint_frequency: 100        # Save every 100 steps
  validation_frequency: 50         # Validate every 50 steps
  save_strategy: "steps"           # Save based on steps (not epochs)

# NOTE: v0.64.5+ uses constant learning_rate, no scheduler_config needed

eval_config:
  repetition_penalty: 1.0
  temperature: 0.0                 # Greedy decoding for validation
  top_k: 50
  top_p: 1.0

device_config:
  enable_ddp: True                 # N300: Dual chips with DDP
  mesh_shape: [1, 2]               # 1x2 mesh (two devices)
